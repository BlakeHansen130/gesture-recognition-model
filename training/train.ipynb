{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c2910d1-56df-461d-ae80-484cb7c22397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec213b9-5f9c-4b09-a626-02ef45baf052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据形状:\n",
      "X_train shape: (12800, 96, 96)\n",
      "y_train shape: (12800,)\n",
      "\n",
      "PyTorch格式数据形状:\n",
      "X_train shape: (12800, 1, 96, 96)\n",
      "LightGestureNet(\n",
      "  (first): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6(inplace=True)\n",
      "  )\n",
      "  (layers): Sequential(\n",
      "    (0): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=1)\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=32, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "验证单个样例输入输出:\n",
      "输入形状: torch.Size([1, 1, 96, 96])\n",
      "输出形状: torch.Size([1, 8])\n",
      "\n",
      "模型详细参数信息:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 48, 48]             144\n",
      "       BatchNorm2d-2           [-1, 16, 48, 48]              32\n",
      "             ReLU6-3           [-1, 16, 48, 48]               0\n",
      "            Conv2d-4           [-1, 96, 48, 48]           1,536\n",
      "       BatchNorm2d-5           [-1, 96, 48, 48]             192\n",
      "             ReLU6-6           [-1, 96, 48, 48]               0\n",
      "            Conv2d-7           [-1, 96, 24, 24]             864\n",
      "       BatchNorm2d-8           [-1, 96, 24, 24]             192\n",
      "             ReLU6-9           [-1, 96, 24, 24]               0\n",
      "           Conv2d-10           [-1, 24, 24, 24]           2,304\n",
      "      BatchNorm2d-11           [-1, 24, 24, 24]              48\n",
      " InvertedResidual-12           [-1, 24, 24, 24]               0\n",
      "           Conv2d-13          [-1, 144, 24, 24]           3,456\n",
      "      BatchNorm2d-14          [-1, 144, 24, 24]             288\n",
      "            ReLU6-15          [-1, 144, 24, 24]               0\n",
      "           Conv2d-16          [-1, 144, 24, 24]           1,296\n",
      "      BatchNorm2d-17          [-1, 144, 24, 24]             288\n",
      "            ReLU6-18          [-1, 144, 24, 24]               0\n",
      "           Conv2d-19           [-1, 24, 24, 24]           3,456\n",
      "      BatchNorm2d-20           [-1, 24, 24, 24]              48\n",
      " InvertedResidual-21           [-1, 24, 24, 24]               0\n",
      "           Conv2d-22          [-1, 144, 24, 24]           3,456\n",
      "      BatchNorm2d-23          [-1, 144, 24, 24]             288\n",
      "            ReLU6-24          [-1, 144, 24, 24]               0\n",
      "           Conv2d-25          [-1, 144, 12, 12]           1,296\n",
      "      BatchNorm2d-26          [-1, 144, 12, 12]             288\n",
      "            ReLU6-27          [-1, 144, 12, 12]               0\n",
      "           Conv2d-28           [-1, 32, 12, 12]           4,608\n",
      "      BatchNorm2d-29           [-1, 32, 12, 12]              64\n",
      " InvertedResidual-30           [-1, 32, 12, 12]               0\n",
      "           Conv2d-31          [-1, 192, 12, 12]           6,144\n",
      "      BatchNorm2d-32          [-1, 192, 12, 12]             384\n",
      "            ReLU6-33          [-1, 192, 12, 12]               0\n",
      "           Conv2d-34          [-1, 192, 12, 12]           1,728\n",
      "      BatchNorm2d-35          [-1, 192, 12, 12]             384\n",
      "            ReLU6-36          [-1, 192, 12, 12]               0\n",
      "           Conv2d-37           [-1, 32, 12, 12]           6,144\n",
      "      BatchNorm2d-38           [-1, 32, 12, 12]              64\n",
      " InvertedResidual-39           [-1, 32, 12, 12]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 32, 1, 1]               0\n",
      "          Flatten-41                   [-1, 32]               0\n",
      "           Linear-42                    [-1, 8]             264\n",
      "================================================================\n",
      "Total params: 39,256\n",
      "Trainable params: 39,256\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 15.45\n",
      "Params size (MB): 0.15\n",
      "Estimated Total Size (MB): 15.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import model  # 这里的 model 是你定义模型的那个 .ipynb 文件的名称（去掉 .ipynb 后缀）\n",
    "from model import LightGestureNet\n",
    "from model import InvertedResidual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91326e4b-9282-4fe1-b88f-8488ddfeac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        # 确保数据格式正确\n",
    "        self.X = torch.FloatTensor(X).unsqueeze(1)  # 添加通道维度\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "        \n",
    "# 添加更多正则化\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(30),  # 增大旋转角度\n",
    "    transforms.RandomAffine(0, scale=(0.8, 1.2)),  # 增大缩放范围\n",
    "    transforms.RandomAffine(0, translate=(0.2, 0.2)),  # 增大平移范围\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95293c3b-e4f0-446b-8e43-7cbd61a05160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 12800\n",
      "验证集大小: 1600\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "with open('train.pkl', 'rb') as f:\n",
    "    X_train, y_train = pickle.load(f)\n",
    "with open('test.pkl', 'rb') as f:\n",
    "    X_val, y_val = pickle.load(f)\n",
    "\n",
    "# 创建数据集实例\n",
    "train_dataset = GestureDataset(X_train, y_train, transform=train_transform)\n",
    "val_dataset = GestureDataset(X_val, y_val)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "print(\"训练集大小:\", len(train_dataset))\n",
    "print(\"验证集大小:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27aab9f8-243b-431a-ae6f-6f86663525ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "# 自定义权重初始化函数\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        # 使用 He (Kaiming) 初始化卷积层的权重\n",
    "        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        # 使用常量初始化批归一化层\n",
    "        init.constant_(m.weight, 1)\n",
    "        init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        # 使用 Xavier 初始化全连接层的权重\n",
    "        init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44258f8e-582e-4922-9432-7310e7f2df73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "# 检查CUDA可用性并设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 加载模型并移至设备\n",
    "model = LightGestureNet().to(device)\n",
    "\n",
    "# 设置训练参数\n",
    "num_epochs = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.apply(weight_init)  # 添加权重初始化\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "# 用于早停的变量\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f8742cd-95db-48eb-b813-9725e91bb027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|█████████████████████████████| 400/400 [00:16<00:00, 23.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50:\n",
      "Train Loss: 0.7962, Train Acc: 76.95%\n",
      "Val Loss: 0.1684, Val Acc: 97.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|█████████████████████████████| 400/400 [00:17<00:00, 23.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50:\n",
      "Train Loss: 0.1233, Train Acc: 98.26%\n",
      "Val Loss: 0.0255, Val Acc: 99.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|█████████████████████████████| 400/400 [00:17<00:00, 22.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50:\n",
      "Train Loss: 0.0570, Train Acc: 99.18%\n",
      "Val Loss: 0.0247, Val Acc: 99.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|█████████████████████████████| 400/400 [00:18<00:00, 22.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50:\n",
      "Train Loss: 0.0308, Train Acc: 99.52%\n",
      "Val Loss: 0.0138, Val Acc: 99.81%\n",
      "\n",
      "达到目标准确率! 训练终止于epoch 4\n"
     ]
    }
   ],
   "source": [
    "# 记录训练历史\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练模式\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    # 训练循环\n",
    "    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    # 计算训练指标\n",
    "    epoch_train_loss = train_loss / len(train_loader)\n",
    "    epoch_train_acc = 100. * train_correct / train_total\n",
    "    \n",
    "    # 验证模式\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    # 计算验证指标\n",
    "    epoch_val_loss = val_loss / len(val_loader)\n",
    "    epoch_val_acc = 100. * val_correct / val_total\n",
    "    \n",
    "    # 更新学习率\n",
    "    scheduler.step()\n",
    "    \n",
    "    # 记录历史\n",
    "    history['train_loss'].append(epoch_train_loss)\n",
    "    history['train_acc'].append(epoch_train_acc)\n",
    "    history['val_loss'].append(epoch_val_loss)\n",
    "    history['val_acc'].append(epoch_val_acc)\n",
    "    \n",
    "    print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%')\n",
    "    print(f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%')\n",
    "\n",
    "    # 在训练循环开始前定义阈值\n",
    "    ACCURACY_THRESHOLD = 99.27\n",
    "\n",
    "    # 在每个epoch结束后的部分增加判断\n",
    "    if epoch_train_acc >= ACCURACY_THRESHOLD and epoch_val_acc >= ACCURACY_THRESHOLD:\n",
    "        print(f'\\n达到目标准确率! 训练终止于epoch {epoch+1}')\n",
    "        break\n",
    "    \n",
    "    # 早停检查\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        patience_counter = 0\n",
    "        # 保存最佳模型\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\n早停: 验证损失在 {patience} 个epoch内没有改善')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d366c7d-fbb1-4d65-b475-cec3db0ee185",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# 绘制损失\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Training Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 绘制准确率\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Training Accuracy')\n",
    "plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "668baf27-0cdd-4e0b-bcb4-ffcdc0e9151e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX 模型推理测试成功，结果： [array([[-22.816376 , -14.589228 , -36.480556 , -14.02177  ,  -4.8359632,\n",
      "         68.22424  ,  40.38867  ,  27.884693 ]], dtype=float32)]\n",
      "INFO:tensorflow:Assets written to: tf_gesture_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_gesture_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SavedModel 已成功生成：tf_gesture_model\n",
      "模型已保存为以下格式：\n",
      "- PyTorch (.pth)\n",
      "- ONNX (.onnx)\n",
      "- TFLite (.tflite)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1732715916.408692 1020467 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1732715916.408710 1020467 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2024-11-27 21:58:36.408859: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: tf_gesture_model\n",
      "2024-11-27 21:58:36.409226: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-11-27 21:58:36.409235: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: tf_gesture_model\n",
      "2024-11-27 21:58:36.411429: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-11-27 21:58:36.427605: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: tf_gesture_model\n",
      "2024-11-27 21:58:36.431807: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 22953 microseconds.\n",
      "2024-11-27 21:58:36.466284: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 24.552 M  ops, equivalently 12.276 M  MACs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 强制使用 CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 加载最佳模型并放置到 CPU 上\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device, weights_only=False))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 保存为PyTorch格式\n",
    "torch.save(model.state_dict(), 'gesture_model.pth')\n",
    "\n",
    "# 导出为ONNX格式\n",
    "dummy_input = torch.randn(1, 1, 96, 96).to(device)  # 确保输入张量在 CPU 上\n",
    "onnx_model_path = 'gesture_model.onnx'\n",
    "torch.onnx.export(model, dummy_input, onnx_model_path,\n",
    "                  input_names=['input'],\n",
    "                  output_names=['output'],\n",
    "                  dynamic_axes={'input': {0: 'batch_size'},\n",
    "                                'output': {0: 'batch_size'}})\n",
    "\n",
    "# 使用 ONNX Runtime 进行推理验证 ONNX 模型的正确性\n",
    "try:\n",
    "    ort_session = ort.InferenceSession(onnx_model_path)\n",
    "    outputs = ort_session.run(None, {'input': dummy_input.numpy()})\n",
    "    print(\"ONNX 模型推理测试成功，结果：\", outputs)\n",
    "except Exception as e:\n",
    "    print(\"Error during ONNX model inference:\", e)\n",
    "    exit(1)\n",
    "\n",
    "# 手动构建等价的 TensorFlow 模型以便转换为 SavedModel\n",
    "class SimpleKerasModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SimpleKerasModel, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu')\n",
    "        self.pool1 = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')\n",
    "        self.pool2 = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(10)  # 假设有 10 个类\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# 创建 TensorFlow 模型并保存为 SavedModel 格式\n",
    "try:\n",
    "    tf_model = SimpleKerasModel()\n",
    "    tf_input = tf.convert_to_tensor(np.random.randn(1, 96, 96, 1), dtype=tf.float32)\n",
    "    tf_model(tf_input)  # 通过调用一次模型来构建计算图\n",
    "    tf.saved_model.save(tf_model, 'tf_gesture_model')\n",
    "    print(f\"SavedModel 已成功生成：tf_gesture_model\")\n",
    "except Exception as e:\n",
    "    print(\"Error during manual TensorFlow model creation:\", e)\n",
    "    exit(1)\n",
    "\n",
    "# 创建 TFLite 转换器\n",
    "try:\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model('tf_gesture_model')\n",
    "\n",
    "    # 设置优化选项\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_types = [tf.float32]\n",
    "\n",
    "    # 执行转换\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # 保存 TFLite 模型\n",
    "    with open('gesture_model.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "    print(\"模型已保存为以下格式：\")\n",
    "    print(\"- PyTorch (.pth)\")\n",
    "    print(\"- ONNX (.onnx)\")\n",
    "    print(\"- TFLite (.tflite)\")\n",
    "except Exception as e:\n",
    "    print(\"Error during SavedModel to TFLite conversion:\", e)\n",
    "    exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python dl_env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
